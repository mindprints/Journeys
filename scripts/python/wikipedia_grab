#!/usr/bin/env python3
"""
Wikipedia to AI Poster Generator
Fetches AI pioneers and models from Wikipedia and generates poster JSON files.
"""

import argparse
import requests
import json
import uuid
from datetime import datetime
from pathlib import Path
import time
import re
import unicodedata

# Configuration
OUTPUT_DIR = Path("ai_posters")
EXISTING_POSTER_ROOTS = [Path("JSON_Posters"), Path("backups")]
EXCLUDE_DIR_NAMES = {"poster_schemas", "Journeys"}
MERGE_ENRICH = True
MERGE_LOG_PATH = OUTPUT_DIR / "merge_enrichment.log"
DELAY_BETWEEN_REQUESTS = 1  # seconds, to be respectful to Wikipedia's servers


def parse_args():
    parser = argparse.ArgumentParser(description="Wikipedia to AI Poster Generator")
    parser.add_argument("--category", help="Custom category label")
    parser.add_argument("--topics", help="Comma-separated Wikipedia topics")
    parser.add_argument("--count", type=int, help="Limit number of topics to process")
    parser.add_argument("--merge-enrich", choices=["true", "false"], help="Enable merge enrichment")
    parser.add_argument("--merge-only", choices=["true", "false"], help="Skip creating new posters")
    return parser.parse_args()

# Topics to fetch
AI_PIONEERS = [
    "Alan_Turing",
    "John_McCarthy_(computer_scientist)",
    "Marvin_Minsky",
    "Geoffrey_Hinton",
    "Yann_LeCun",
    "Yoshua_Bengio",
    "Andrew_Ng",
    "Fei-Fei_Li",
    "Demis_Hassabis",
    "Ilya_Sutskever",
    "Jurgen_Schmidhuber",
    "Stuart_Russell",
    "Peter_Norvig",
    "Judea_Pearl",
    "Michael_I._Jordan",
    "Ian_Goodfellow",
    "Andrej_Karpathy",
    "Daphne_Koller",
    "Jeff_Dean_(computer_scientist)",
    "Turing_Award"  # Include the award itself
]

AI_MODELS = [
    "Perceptron",
    "Backpropagation",
    "Convolutional_neural_network",
    "Recurrent_neural_network",
    "Long_short-term_memory",
    "Generative_adversarial_network",
    "Transformer_(machine_learning_model)",
    "BERT_(language_model)",
    "GPT-3",
    "GPT-4",
    "Diffusion_model",
    "Autoencoder",
    "ResNet",
    "AlexNet",
    "U-Net",
    "DALL-E",
    "Stable_Diffusion",
    "Neural_Turing_machine",
    "Attention_(machine_learning)",
    "Word2vec"
]

AI_CONCEPTS = [
    "Artificial_intelligence",
    "Machine_learning",
    "Deep_learning",
    "Neural_network",
    "Reinforcement_learning",
    "Supervised_learning",
    "Unsupervised_learning",
    "Natural_language_processing",
    "Computer_vision",
    "Transfer_learning",
    "Few-shot_learning",
    "Artificial_neural_network",
    "Gradient_descent",
    "Overfitting",
    "Regularization_(mathematics)",
    "Batch_normalization",
    "Dropout_(neural_networks)",
    "Activation_function",
    "Loss_function",
    "Mathematical_optimization"
]

AI_COMPANIES = [
    "OpenAI",
    "DeepMind",
    "Anthropic",
    "Google_AI",
    "Meta_AI",
    "IBM_Watson",
    "Microsoft_Research",
    "Allen_Institute_for_AI",
    "Hugging_Face",
    "Stability_AI"
]

AI_LANDMARKS = [
    "Dartmouth_workshop",
    "ImageNet",
    "AlphaGo",
    "Deep_Blue_(chess_computer)",
    "Watson_(computer)",
    "ChatGPT",
    "AI_winter",
    "Turing_test",
    "Chinese_room",
    "AI_alignment"
]


def fetch_wikipedia_summary(topic):
    """Fetch summary data from Wikipedia REST API"""
    url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{topic}"
    headers = {
        'User-Agent': 'AI-Poster-Generator/1.0 (Educational Project; mindp@example.com)'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f"Error fetching {topic}: {e}")
        return None


def extract_year_from_text(text):
    """Extract a year from text (for chronology)"""
    # Look for 4-digit years
    years = re.findall(r'\b(19\d{2}|20\d{2})\b', text)
    return int(years[0]) if years else None


def normalize_text(value):
    if not value:
        return ""
    return re.sub(r"[^a-z0-9]+", "", value.lower())


def normalize_url(value):
    if not value:
        return ""
    normalized = value.strip().lower()
    return normalized[:-1] if normalized.endswith("/") else normalized


def to_ascii(value):
    if value is None:
        return ""
    normalized = unicodedata.normalize("NFKD", value)
    return normalized.encode("ascii", "ignore").decode("ascii")


def collect_existing_poster_keys(poster):
    keys = {
        "titles": set(),
        "tags": set(),
        "sources": set(),
        "links": set(),
    }

    front = poster.get("front", {})
    meta = poster.get("meta", {})
    back = poster.get("back", {})

    title = front.get("title")
    if title:
        keys["titles"].add(normalize_text(title))

    for tag in meta.get("tags", []) or []:
        keys["tags"].add(normalize_text(tag))

    source = meta.get("source")
    if source:
        keys["sources"].add(normalize_url(source))

    for link in back.get("links", []) or []:
        url = link.get("url")
        if url:
            keys["links"].add(normalize_url(url))

    return keys


def build_existing_index():
    index = {
        "titles": set(),
        "tags": set(),
        "sources": set(),
        "links": set(),
    }
    lookup = {
        "titles": {},
        "tags": {},
        "sources": {},
        "links": {},
    }

    for root_dir in EXISTING_POSTER_ROOTS:
        if not root_dir.exists():
            continue

        for file_path in root_dir.rglob("*.json"):
            if any(part in EXCLUDE_DIR_NAMES for part in file_path.parts):
                continue
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    poster = json.load(f)
            except (OSError, json.JSONDecodeError):
                continue

            keys = collect_existing_poster_keys(poster)
            for key, values in keys.items():
                index[key].update(values)
                for value in values:
                    if value and value not in lookup[key]:
                        lookup[key][value] = file_path

    return index, lookup


def find_duplicate_reason(data, topic, existing_index):
    title = data.get("title", "")
    url = data.get("content_urls", {}).get("desktop", {}).get("page", "")

    normalized_title = normalize_text(title)
    normalized_topic = normalize_text(topic.replace("_", " "))
    normalized_url = normalize_url(url)

    if normalized_url and (
        normalized_url in existing_index["sources"]
        or normalized_url in existing_index["links"]
    ):
        return "source"

    if normalized_title and (
        normalized_title in existing_index["titles"]
        or normalized_title in existing_index["tags"]
    ):
        return "title"

    if normalized_topic and normalized_topic in existing_index["tags"]:
        return "tag"

    return None


def find_existing_match_path(poster, topic, existing_lookup):
    front = poster.get("front", {})
    meta = poster.get("meta", {})
    back = poster.get("back", {})

    title = front.get("title", "")
    source = meta.get("source", "")

    normalized_title = normalize_text(title)
    normalized_topic = normalize_text(topic.replace("_", " "))
    normalized_source = normalize_url(source)

    if normalized_source:
        if normalized_source in existing_lookup["sources"]:
            return existing_lookup["sources"][normalized_source]
        if normalized_source in existing_lookup["links"]:
            return existing_lookup["links"][normalized_source]

    for link in back.get("links", []) or []:
        url = normalize_url(link.get("url", ""))
        if url in existing_lookup["sources"]:
            return existing_lookup["sources"][url]
        if url in existing_lookup["links"]:
            return existing_lookup["links"][url]

    if normalized_title:
        if normalized_title in existing_lookup["titles"]:
            return existing_lookup["titles"][normalized_title]
        if normalized_title in existing_lookup["tags"]:
            return existing_lookup["tags"][normalized_title]

    if normalized_topic and normalized_topic in existing_lookup["tags"]:
        return existing_lookup["tags"][normalized_topic]

    return None


def determine_category(topic_list_name, category_label=None):
    """Determine category based on which list the topic came from"""
    if category_label:
        return [category_label]

    categories_map = {
        'pioneers': ['AI Pioneers', 'People'],
        'models': ['AI Models', 'Machine Learning'],
        'concepts': ['AI Concepts', 'Theory'],
        'companies': ['AI Companies', 'Organizations'],
        'landmarks': ['AI History', 'Milestones']
    }
    return categories_map.get(topic_list_name, ['Artificial Intelligence'])


def create_poster_from_wikipedia(topic, category_type, existing_index, category_label=None):
    """Convert Wikipedia data to poster schema v2"""
    data = fetch_wikipedia_summary(topic)
    
    if not data:
        return None, None
    
    duplicate_reason = find_duplicate_reason(data, topic, existing_index)

    # Extract basic info
    title = data.get('title', '')
    description = data.get('description', '')
    extract = data.get('extract', '')
    
    # Create subtitle
    subtitle = description if description else extract[:100] + '...' if len(extract) > 100 else extract
    
    # Build the poster object
    poster = {
        "version": 2,
        "type": "poster-v2",
        "uid": str(uuid.uuid4()),
        "front": {
            "title": title,
            "subtitle": subtitle
        },
        "back": {
            "layout": "image-top" if data.get('thumbnail') else "text-only",
            "text": extract,
            "links": [
                {
                    "type": "external",
                    "label": "Read more on Wikipedia",
                    "url": data.get('content_urls', {}).get('desktop', {}).get('page', ''),
                    "primary": True
                }
            ]
        },
        "meta": {
            "created": datetime.now().isoformat(),
            "modified": datetime.now().isoformat(),
            "categories": determine_category(category_type, category_label),
            "tags": [topic.replace('_', ' ')],
            "source": data.get('content_urls', {}).get('desktop', {}).get('page', '')
        }
    }
    
    # Add image if available
    if data.get('thumbnail'):
        poster['back']['image'] = {
            "src": data['thumbnail'].get('source', ''),
            "alt": title,
            "position": "top"
        }
    
    # Add chronology for pioneers (people)
    if category_type == 'pioneers' and 'extract' in data:
        year = extract_year_from_text(extract)
        if year:
            poster['front']['chronology'] = {
                "epochStart": year,
                "epochEnd": datetime.now().year,
                "epochEvents": [
                    {
                        "year": year,
                        "name": f"Birth/Founding of {title}"
                    }
                ]
            }
    
    return poster, duplicate_reason


def save_poster(poster, filename):
    """Save poster to JSON file"""
    OUTPUT_DIR.mkdir(exist_ok=True)
    filepath = OUTPUT_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(poster, f, indent=2, ensure_ascii=True)
    
    print(f"Created: {filename}")


def save_existing_poster(filepath, poster):
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(poster, f, indent=2, ensure_ascii=True)


def merge_enrich_poster(existing_poster, new_poster):
    changed = False

    existing_front = existing_poster.setdefault("front", {})
    existing_back = existing_poster.setdefault("back", {})
    existing_meta = existing_poster.setdefault("meta", {})

    new_front = new_poster.get("front", {})
    new_back = new_poster.get("back", {})
    new_meta = new_poster.get("meta", {})

    if not existing_front.get("subtitle") and new_front.get("subtitle"):
        existing_front["subtitle"] = new_front["subtitle"]
        changed = True

    if not existing_back.get("text") and new_back.get("text"):
        existing_back["text"] = new_back["text"]
        changed = True

    if not existing_back.get("image") and new_back.get("image"):
        existing_back["image"] = new_back["image"]
        changed = True

    existing_links = existing_back.get("links") or []
    existing_back["links"] = existing_links
    new_links = new_back.get("links") or []
    existing_link_urls = {normalize_url(link.get("url", "")) for link in existing_links}
    for link in new_links:
        url = normalize_url(link.get("url", ""))
        if url and url not in existing_link_urls:
            existing_links.append(link)
            existing_link_urls.add(url)
            changed = True

    if not existing_meta.get("source") and new_meta.get("source"):
        existing_meta["source"] = new_meta["source"]
        changed = True

    existing_tags = existing_meta.get("tags") or []
    existing_meta["tags"] = existing_tags
    existing_tag_keys = {normalize_text(tag) for tag in existing_tags}
    for tag in new_meta.get("tags") or []:
        normalized_tag = normalize_text(tag)
        if normalized_tag and normalized_tag not in existing_tag_keys:
            existing_tags.append(tag)
            existing_tag_keys.add(normalized_tag)
            changed = True

    if changed:
        existing_meta["modified"] = datetime.now().isoformat()

    return changed


def generate_all_posters(merge_enrich=True, merge_only=False, category_label=None, topics_override=None, count=None):
    """Generate all posters from the topic lists"""
    existing_index, existing_lookup = build_existing_index()

    if topics_override:
        topics = [topic for topic in topics_override if topic]
        if count:
            topics = topics[:count]
        category_prefix = normalize_text(category_label or "category") or "category"
        all_topics = [
            (category_prefix, topics, category_label)
        ]
    else:
        all_topics = [
            ('pioneers', AI_PIONEERS, None),
            ('models', AI_MODELS, None),
            ('concepts', AI_CONCEPTS, None),
            ('companies', AI_COMPANIES, None),
            ('landmarks', AI_LANDMARKS, None)
        ]
    
    total_created = 0
    total_failed = 0
    total_skipped = 0
    total_merged = 0
    merged_paths = []
    
    print(f"Starting poster generation...")
    print(f"Output directory: {OUTPUT_DIR.absolute()}\n")
    
    for category_type, topics, category_label_item in all_topics:
        display_label = category_label_item or category_type
        print(f"\n{'='*60}")
        print(f"Processing {display_label.upper()} ({len(topics)} topics)")
        print(f"{'='*60}")
        
        for i, topic in enumerate(topics, 1):
            print(f"[{i}/{len(topics)}] Fetching: {topic}... ", end='')
            
            poster, duplicate_reason = create_poster_from_wikipedia(
                topic,
                category_type,
                existing_index,
                category_label=category_label_item
            )

            if poster and not duplicate_reason:
                if merge_only:
                    print("SKIP create (merge-only)")
                    total_skipped += 1
                    continue
                # Create safe filename
                safe_topic = topic.replace('/', '_').replace('(', '').replace(')', '')
                safe_topic = to_ascii(safe_topic)
                if not safe_topic:
                    safe_topic = normalize_text(topic) or "topic"
                filename = f"{category_type}_{safe_topic}.json"
                save_poster(poster, filename)
                total_created += 1
                existing_index["titles"].add(normalize_text(poster.get("front", {}).get("title")))
                source = normalize_url(poster.get("meta", {}).get("source"))
                if source:
                    existing_index["sources"].add(source)
                for tag in poster.get("meta", {}).get("tags", []) or []:
                    existing_index["tags"].add(normalize_text(tag))
            elif duplicate_reason:
                match_path = find_existing_match_path(poster, topic, existing_lookup)
                if merge_enrich and match_path:
                    try:
                        with open(match_path, "r", encoding="utf-8") as f:
                            existing_poster = json.load(f)
                    except (OSError, json.JSONDecodeError):
                        print(f"SKIP duplicate ({duplicate_reason})")
                        total_skipped += 1
                    else:
                        if merge_enrich_poster(existing_poster, poster):
                            save_existing_poster(match_path, existing_poster)
                            print(f"MERGE enriched ({duplicate_reason})")
                            total_merged += 1
                            merged_paths.append(str(match_path))
                        else:
                            print(f"SKIP duplicate ({duplicate_reason})")
                            total_skipped += 1
                else:
                    print(f"SKIP duplicate ({duplicate_reason})")
                    total_skipped += 1
            else:
                print(f"Failed")
                total_failed += 1
            
            # Be respectful to Wikipedia's servers
            if i < len(topics):
                time.sleep(DELAY_BETWEEN_REQUESTS)
    
    print(f"\n{'='*60}")
    print(f"SUMMARY")
    print(f"{'='*60}")
    print(f"Created: {total_created} posters")
    print(f"SKIP duplicates: {total_skipped} posters")
    print(f"MERGE enriched: {total_merged} posters")
    print(f"Failed: {total_failed} posters")
    print(f"Output directory: {OUTPUT_DIR.absolute()}")
    print(f"\nTotal topics attempted: {total_created + total_failed + total_skipped + total_merged}")

    if merged_paths:
        OUTPUT_DIR.mkdir(exist_ok=True)
        with open(MERGE_LOG_PATH, "a", encoding="utf-8") as log_file:
            log_file.write(f"{datetime.now().isoformat()}\n")
            for path in merged_paths:
                log_file.write(f"{path}\n")
            log_file.write("\n")
        print(f"Merge log: {MERGE_LOG_PATH.absolute()}")


def main():
    """Main entry point"""
    args = parse_args()
    merge_enrich = MERGE_ENRICH
    merge_only = False

    if args.merge_enrich is not None:
        merge_enrich = args.merge_enrich == "true"
    if args.merge_only is not None:
        merge_only = args.merge_only == "true"

    topics_override = None
    if args.topics:
        topics_override = [
            topic.strip()
            for topic in re.split(r"[\n,]", args.topics)
            if topic.strip()
        ]

    print("="*60)
    print("AI POSTER GENERATOR")
    print("Fetching data from Wikipedia API")
    print("="*60)
    
    generate_all_posters(
        merge_enrich=merge_enrich,
        merge_only=merge_only,
        category_label=args.category,
        topics_override=topics_override,
        count=args.count
    )
    
    print("\nDone!")


if __name__ == "__main__":
    main()
